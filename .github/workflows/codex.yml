name: "Codex"

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "実行モード: full(全生成) / incremental(差分)"
        required: false
        default: "full"
        type: choice
        options: ["full", "incremental"]
      model:
        description: "OpenAI モデル名 (例: gpt-5-codex / gpt-5 / gpt-4.1 / gpt-4o-mini)"
        required: false
        default: ""
        type: string

  push:
    branches: ["main"]
    paths:
      - "spec/**"

permissions:
  contents: write
  pull-requests: write

jobs:
  generate:
    if: ${{ github.event_name != 'push' || !contains(github.event.head_commit.message, '[skip-codex]') }}
    runs-on: ubuntu-latest

    env:
      SPEC_ROOT: "spec"
      SPEC_DIRS: "foundation env-profiles features tests"
      PROMPT_FILE: "ai/prompt.md"
      PATCH_FILE: "ai/patch.diff"
      MODEL_DEFAULT: "gpt-4o-mini"
      OPENAI_MODEL: "${{ inputs.model }}"
      MODE: "${{ inputs.mode }}"
      OPENAI_API_KEY: "${{ secrets.OPENAI_API_KEY }}"
      OPENAI_BASE_URL: "${{ vars.OPENAI_BASE_URL }}"
      # 生成先を固定（ここに最低1ファイル以上を必ず作る）
      GEN_OUTPUT_DIR: "generated"

    steps:
      - name: "Checkout"
        uses: actions/checkout@v4

      - name: "Setup Node (cache if lock exists)"
        if: ${{ hashFiles('**/package-lock.json', '**/npm-shrinkwrap.json', '**/yarn.lock') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: "Setup Node (no cache)"
        if: ${{ hashFiles('**/package-lock.json', '**/npm-shrinkwrap.json', '**/yarn.lock') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: "Preflight: API key"
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${OPENAI_API_KEY:-}" ]; then
            echo "::error::OPENAI_API_KEY is not set"
            exit 1
          fi
          mkdir -p ai "${GEN_OUTPUT_DIR}"

      - name: "Compose prompt from spec (no-heredoc)"
        shell: bash
        run: |
          set -euo pipefail
          : > "$PROMPT_FILE"
          printf "%s\n" "# Codex generation prompt" >> "$PROMPT_FILE"
          printf "%s\n" "" >> "$PROMPT_FILE"
          printf "%s\n" "## Mode: ${MODE}" >> "$PROMPT_FILE"
          printf "%s\n" "" >> "$PROMPT_FILE"

          # 変更ファイル拾い（incremental） or 全文
          tmp_list="$(mktemp)"
          if [ "${MODE:-full}" = "incremental" ]; then
            git fetch --depth=50 origin || true
            git diff --name-only --diff-filter=AMR origin/main...HEAD -- "${SPEC_ROOT}/" \
              | grep -E '\.(md|txt|ya?ml|json)$' >> "$tmp_list" || true
          fi
          if [ ! -s "$tmp_list" ]; then
            for d in $SPEC_DIRS; do
              if [ -d "${SPEC_ROOT}/${d}" ]; then
                find "${SPEC_ROOT}/${d}" -type f \
                  \( -name "*.md" -o -name "*.txt" -o -name "*.yaml" -o -name "*.yml" -o -name "*.json" \) \
                  >> "$tmp_list"
              fi
            done
          fi

          if [ -s "$tmp_list" ]; then
            LC_ALL=C sort "$tmp_list" -o "$tmp_list"
            while IFS= read -r f; do
              printf "%s\n" "" >> "$PROMPT_FILE"
              printf "%s\n" "### $(basename "$f")" >> "$PROMPT_FILE"
              printf "%s\n" "" >> "$PROMPT_FILE"
              sed -e 's/\r$//' "$f" >> "$PROMPT_FILE"
              printf "%s\n" "" >> "$PROMPT_FILE"
            done < "$tmp_list"
          fi
          rm -f "$tmp_list"

          # タスク契約：常に何かを生成する
          {
            printf "\n## TASK CONTRACT (MUST)\n\n"
            printf -- "- Read the specs above and *generate implementation code* and/or *tests*.\n"
            printf -- "- You MUST produce at least one file change.\n"
            printf -- "- Default output location is '%s/'. Create new files there unless the spec explicitly maps to existing paths.\n" "${GEN_OUTPUT_DIR}"
            printf -- "- Prefer TypeScript/Node.js 20 if language is not specified; otherwise follow the spec.\n"
            printf -- "- Use clean structure, minimal dependencies, and include brief file-level comments (in English or Japanese per spec).\n"
            printf -- "- If information is missing, generate a best-effort scaffold with TODO comments.\n"
          } >> "$PROMPT_FILE"

          # 出力契約：必ず diff、no-op 禁止
          {
            printf "\n## OUTPUT CONTRACT (STRICT)\n\n"
            printf -- "- Return a single Unified Diff patch rooted at the repository.\n"
            printf -- "- No prose, no explanations, no code fences, no headings.\n"
            printf -- "- Begin with 'diff --git' or with '--- a/' then '+++ b/'.\n"
            printf -- "- DO NOT return a no-op. If unsure, synthesize a minimal scaffold under '%s/'.\n" "${GEN_OUTPUT_DIR}"
          } >> "$PROMPT_FILE"

      - name: "Call OpenAI (curl + jq, no heredoc) — auto route with retry"
        id: call
        shell: bash
        run: |
          set -euo pipefail

          is_patch() { grep -Eq '^(diff --git|--- a/)' "$1" && grep -Eq '(^\\+\\+\\+ b/|^diff --git)' "$1"; }

          _in="${OPENAI_MODEL:-}"
          MODEL="$MODEL_DEFAULT"
          if [ -n "$_in" ]; then
            case "$_in" in gpt-*|o* ) MODEL="$_in" ;; * ) echo "note: unknown model '$_in' -> fallback to $MODEL_DEFAULT" ;; esac
          fi
          BASE="${OPENAI_BASE_URL:-https://api.openai.com}"
          echo "Using model: $MODEL"
          echo "Base URL  : $BASE"

          SYSTEM_CONTRACT=$'You are a patch generator. Always output a unified diff (UTF-8) rooted at repository with at least one file changed. Default output directory is '"${GEN_OUTPUT_DIR}"$'/ . Do not return a no-op. If spec is ambiguous, generate a minimal scaffold under that directory with TODO comments. No prose, no code fences.'

          # --- build request（Responses: temperature を送らない）---
          if printf "%s" "$MODEL" | grep -qiE '^gpt-5-codex( |$|-|:)'; then
            jq -n --arg model "$MODEL" --arg sys "$SYSTEM_CONTRACT" --rawfile prompt "$PROMPT_FILE" \
              '{
                 model: $model,
                 max_output_tokens: 4096,
                 input: [
                   { role: "system", content: [ { type: "input_text", text: $sys } ] },
                   { role: "user",   content: [ { type: "input_text", text: $prompt } ] }
                 ]
               }' > ai/request.json
            ENDPOINT="${BASE%/}/v1/responses"
          else
            jq -n --arg model "$MODEL" --arg sys "$SYSTEM_CONTRACT" --rawfile prompt "$PROMPT_FILE" \
              '{
                 model: $model, temperature: 0, max_tokens: 4096,
                 messages: [ { role: "system", content: $sys }, { role: "user", content: $prompt } ]
               }' > ai/request.json
            ENDPOINT="${BASE%/}/v1/chat/completions"
          fi

          # --- call ---
          resp_with_code="$(mktemp)"
          curl -sS -H "Authorization: Bearer ${OPENAI_API_KEY}" -H "Content-Type: application/json" \
               -w "\n%{http_code}\n" -d @ai/request.json "$ENDPOINT" | tee "$resp_with_code" >/dev/null
          status="$(tail -n1 "$resp_with_code")"
          head -n -1 "$resp_with_code" > ai/response.json
          rm -f "$resp_with_code"
          if [ "${status}" -ge 300 ]; then
            echo "::error::OpenAI error ${status}"
            head -c 800 ai/request.json; echo
            cat ai/response.json || true
            exit 3
          fi

          # robust extract（Responses / Chat 兼用）
          EXTRACT_FILTER='[
            .output_text,
            ( try (
                .output
                | if type=="array" then . else [] end
                | map(
                    if has("content") then
                      ( .content
                        | (if type=="array" then map(.text // .string // .content // "") else . end)
                        | (if type=="array" then join("") else ( . // "" ) end)
                      )
                    else (.text // .content // "")
                    end
                  )
                | join("")
              ) catch empty ),
            .choices[0].message.content,
            .choices[0].text
          ] | map(select(type=="string" and . != "")) | .[0] // ""'

          jq -r "${EXTRACT_FILTER}" ai/response.json > ai/out.txt
          sed -E -e '1{/^```[a-zA-Z-]*[[:space:]]*$/d;}' -e '${/^```[[:space:]]*$/d;}' -i ai/out.txt || true
          awk 'f||/^(diff --git|--- a\/)/{f=1; print}' ai/out.txt > "$PATCH_FILE"

          if ! is_patch "$PATCH_FILE"; then
            echo "::warning::first attempt not a unified diff; retry with repair prompt"
            head -c 15000 ai/out.txt > ai/out.trim.txt || true

            REPAIR_SYS=$'You are a patch generator. If the given text already contains a unified diff, extract ONLY the diff. Otherwise, SYNTHESIZE a new diff that creates at least one file under '"${GEN_OUTPUT_DIR}"$'/ (e.g. README.generated.md with a short implementation plan and TODOs). No prose around the diff.'

            if printf "%s" "$MODEL" | grep -qiE '^gpt-5-codex( |$|-|:)'; then
              jq -n --arg model "$MODEL" --arg sys "$REPAIR_SYS" --rawfile prev ai/out.trim.txt \
                '{ model: $model, max_output_tokens: 4096,
                   input: [ { role:"system",content:[{type:"input_text",text:$sys}] },
                            { role:"user",  content:[{type:"input_text",text:$prev}] } ] }' > ai/request.repair.json
            else
              jq -n --arg model "$MODEL" --arg sys "$REPAIR_SYS" --rawfile prev ai/out.trim.txt \
                '{ model: $model, temperature: 0, max_tokens: 2048,
                   messages: [ { role:"system",content:$sys }, { role:"user",content:$prev } ] }' > ai/request.repair.json
            fi

            resp_with_code="$(mktemp)"
            curl -sS -H "Authorization: Bearer ${OPENAI_API_KEY}" -H "Content-Type: application/json" \
                 -w "\n%{http_code}\n" -d @ai/request.repair.json "$ENDPOINT" | tee "$resp_with_code" >/dev/null
            status="$(tail -n1 "$resp_with_code")"
            head -n -1 "$resp_with_code" > ai/response.repair.json
            rm -f "$resp_with_code"

            if [ "${status}" -ge 300 ]; then
              echo "::warning::repair call error ${status} — fallback to local scaffold"
              : > "$PATCH_FILE"
            else
              jq -r "${EXTRACT_FILTER}" ai/response.repair.json > ai/out.repair.txt
              sed -E -e '1{/^```[a-zA-Z-]*[[:space:]]*$/d;}' -e '${/^```[[:space:]]*$/d;}' -i ai/out.repair.txt || true
              awk 'f||/^(diff --git|--- a\/)/{f=1; print}' ai/out.repair.txt > "$PATCH_FILE" || true
            fi
          fi

          # 最終フォールバック：必ず差分を作る（スケルトン README をローカル生成）
          if ! grep -Eq '^(diff --git|--- a/)' "$PATCH_FILE"; then
            echo "::warning::Synthesizing local minimal scaffold"
            TS="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
            mkdir -p "${GEN_OUTPUT_DIR}"
            {
              echo "# Generated Scaffold"
              echo
              echo "- Timestamp: ${TS}"
              echo "- This file is created because the model did not return a diff."
              echo "- TODO: replace with real implementation based on specs."
            } > "${GEN_OUTPUT_DIR}/README.generated.md"

            : > "$PATCH_FILE"
            printf "%s\n" "--- /dev/null" >> "$PATCH_FILE"
            printf "%s\n" "+++ b/${GEN_OUTPUT_DIR}/README.generated.md" >> "$PATCH_FILE"
            printf "%s\n" "@@ -0,0 +1,5 @@" >> "$PATCH_FILE"
            sed 's/^/+ /' "${GEN_OUTPUT_DIR}/README.generated.md" >> "$PATCH_FILE"
          fi

          echo "patch prepared"

      # ---------- apply patch → PR ----------
      - name: "Apply patch to working tree"
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git apply --whitespace=fix "$PATCH_FILE"
          if git diff --quiet; then
            echo "::error::No changes after applying patch."; exit 1
          fi

      - name: "Create Pull Request"
        uses: peter-evans/create-pull-request@v6
        with:
          title: "Codex: generate from specs (mode=${{ env.MODE }}, model=${{ env.OPENAI_MODEL != '' && env.OPENAI_MODEL || env.MODEL_DEFAULT }})"
          body: |
            This PR was created automatically by Codex workflow.
            - Mode: `${{ env.MODE }}`
            - Model: `${{ env.OPENAI_MODEL != '' && env.OPENAI_MODEL || env.MODEL_DEFAULT }}`
            - Output dir: `${{ env.GEN_OUTPUT_DIR }}`
          branch: "codex/autogen-${{ github.run_id }}"
          base: "main"
          delete-branch: true
          labels: "codex, automation"

      # ---------- artifacts ----------
      - name: "Upload response debug (if any)"
        if: ${{ failure() }}
        uses: actions/upload-artifact@v4
        with:
          name: "ai-response-debug"
          path: |
            ai/request.json
            ai/response.json
            ai/out.txt
            ai/request.repair.json
            ai/response.repair.json
            ai/out.repair.txt
          if-no-files-found: ignore

      - name: "Upload prompt & patch (always)"
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: "ai-prompt-and-patch"
          path: |
            ai/prompt.md
            ai/patch.diff
          if-no-files-found: ignore
